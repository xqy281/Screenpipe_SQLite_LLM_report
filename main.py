# main.py
import os
import json
import argparse
import difflib
import sqlite3
import platform
import time
import math
from pathlib import Path
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional

# --- åŠ¨æ€å¯¼å…¥æ£€æŸ¥ ---
try:
    import tiktoken
    from sentence_transformers import SentenceTransformer, util
    import google.generativeai as genai
    from openai import OpenAI
    import transformers
except ImportError as e:
    print(f"ä¾èµ–åº“å¯¼å…¥å¤±è´¥: {e}")
    print("è¯·ç¡®ä¿å·²é€šè¿‡ 'pip install -r requirements.txt' å®‰è£…æ‰€æœ‰ä¾èµ–ã€‚")
    exit(1)


# --- å¸¦æ—¶é—´æˆ³çš„æ—¥å¿—å‡½æ•° ---
def log_with_timestamp(message: str):
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    print(f"[{timestamp}] {message}")


# --- æ¨¡å— 1.6: LLM æ¥å£æ¨¡å— ---
class LLMConnector:
    # ... æ­¤éƒ¨åˆ†ä»£ç æ— å˜åŒ– ...
    def __init__(self, provider_name: str, provider_config: Dict[str, Any]):
        self.provider_name = provider_name
        self.provider_config = provider_config
        self.client = self._initialize_client()
        log_with_timestamp(
            f"ğŸ¤– LLM è¿æ¥å™¨å·²ä¸º provider åˆå§‹åŒ–: {self.provider_name} (æ¨¡å‹: {self.provider_config.get('model_name')})"
        )

    def _initialize_client(self):
        if self.provider_name == "gemini":
            genai.configure(api_key=self.provider_config["api_key"])
            return genai.GenerativeModel(self.provider_config["model_name"])
        elif self.provider_name == "deepseek":
            return OpenAI(
                api_key=self.provider_config["api_key"],
                base_url=self.provider_config["base_url"],
            )
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„ LLM provider: {self.provider_name}")

    def generate(
        self,
        user_prompt: str,
        system_prompt: Optional[str] = None,
        temperature: float = 0.7,
    ) -> str:
        log_with_timestamp(f"ğŸš€ å‘èµ·çœŸå® LLM API è°ƒç”¨ (Temperature: {temperature})...")
        try:
            if self.provider_name == "gemini":
                generation_config = genai.types.GenerationConfig(
                    temperature=temperature
                )
                full_prompt = (
                    f"{system_prompt}\n\n---\n\n{user_prompt}"
                    if system_prompt
                    else user_prompt
                )
                response = self.client.generate_content(
                    full_prompt, generation_config=generation_config
                )
                return response.text
            elif self.provider_name == "deepseek":
                messages = []
                if system_prompt:
                    messages.append({"role": "system", "content": system_prompt})
                messages.append({"role": "user", "content": user_prompt})
                chat_completion = self.client.chat.completions.create(
                    messages=messages,
                    model=self.provider_config["model_name"],
                    temperature=temperature,
                )
                return chat_completion.choices[0].message.content
        except Exception as e:
            log_with_timestamp(f"âŒ LLM API è°ƒç”¨å¤±è´¥: {e}")
            return f"[LLM è°ƒç”¨é”™è¯¯: {e}]"
        return "[LLM è°ƒç”¨è¿”å›ç©º]"


# --- æ¨¡å— 1.1: æ•°æ®è·å–æ¨¡å— ---
class DataFetcher:
    # ... æ­¤éƒ¨åˆ†ä»£ç æ— å˜åŒ– ...
    def __init__(self):
        self.db_path = self._get_db_path()
        if not self.db_path or not self.db_path.exists():
            log_with_timestamp(
                f"âŒ é”™è¯¯: æœªèƒ½æ‰¾åˆ° Screenpipe æ•°æ®åº“ã€‚é¢„æœŸè·¯å¾„: {self.db_path}"
            )
            exit(1)
        log_with_timestamp(f"ğŸ” æˆåŠŸå®šä½ Screenpipe æ•°æ®åº“: {self.db_path}")

    def _get_db_path(self) -> Path | None:
        system = platform.system()
        home = Path.home()
        if system == "Windows":
            path1 = home / ".screenpipe/db.sqlite"
            path2 = home / "AppData/Roaming/Screenpipe/db.sqlite"
            if path1.exists():
                return path1
            return path2
        if system == "Darwin":
            return home / "Library/Application Support/Screenpipe/db.sqlite"
        if system == "Linux":
            return home / ".config/Screenpipe/db.sqlite"
        return None

    def fetch_data(
        self, start_time: datetime, end_time: datetime
    ) -> List[Dict[str, Any]]:
        log_with_timestamp(
            f"ğŸ’¾ æ­£åœ¨ä»æ•°æ®åº“è·å– {start_time.isoformat()} åˆ° {end_time.isoformat()} çš„ OCR æ•°æ®..."
        )
        query = "SELECT ocr.text, frm.timestamp AS captured_at FROM frames AS frm JOIN ocr_text AS ocr ON frm.id = ocr.frame_id WHERE frm.timestamp >= ? AND frm.timestamp <= ? ORDER BY frm.timestamp ASC;"
        records = []
        try:
            with sqlite3.connect(f"file:{self.db_path}?mode=ro", uri=True) as conn:
                conn.row_factory = sqlite3.Row
                cursor = conn.cursor()
                cursor.execute(query, (start_time.isoformat(), end_time.isoformat()))
                for row in cursor.fetchall():
                    records.append(dict(row))
            log_with_timestamp(f"âœ… æˆåŠŸè·å– {len(records)} æ¡ OCR è®°å½•ã€‚")
            return records
        except sqlite3.Error as e:
            log_with_timestamp(f"âŒ æ•°æ®åº“æŸ¥è¯¢å¤±è´¥: {e}")
            return []


# --- æ ¸å¿ƒåº”ç”¨ç±» ---
class DailyReportGenerator:
    def __init__(self, config: Dict[str, Any], cli_args: argparse.Namespace):
        self.config = config
        self.data_fetcher = DataFetcher()
        self.cli_args = cli_args
        self.llm_provider_name = cli_args.llm or self.config["llm_provider"]
        log_with_timestamp(f"ğŸ”§ LLM æä¾›å•†å·²ç¡®å®š: {self.llm_provider_name}")
        self.provider_config = self.config["llm_config"][self.llm_provider_name]
        self.llm_connector = LLMConnector(
            provider_name=self.llm_provider_name, provider_config=self.provider_config
        )
        self.task_template = self._load_task_template(cli_args.task)
        if cli_args.temperature is not None:
            self.temperature = cli_args.temperature
            log_with_timestamp(
                f"ğŸ”§ å·²é€šè¿‡å‘½ä»¤è¡Œå‚æ•°è®¾ç½® Temperature: {self.temperature}"
            )
        else:
            self.temperature = self.provider_config.get("temperature", 0.7)
        log_with_timestamp("æ­£åœ¨åŠ è½½ NLP æ¨¡å‹...")
        self.similarity_model = SentenceTransformer("all-MiniLM-L6-v2")
        self._setup_tokenizers()
        log_with_timestamp("æ¨¡å‹åŠ è½½å®Œæˆã€‚")

    def _load_task_template(self, task_name: str) -> Dict[str, str]:
        templates = self.config.get("prompt_templates", {})
        if task_name not in templates:
            log_with_timestamp(f"âŒ é”™è¯¯: ä»»åŠ¡ '{task_name}' åœ¨é…ç½®æ–‡ä»¶ä¸­æœªå®šä¹‰ã€‚")
            log_with_timestamp(f"å¯ç”¨çš„ä»»åŠ¡æœ‰: {list(templates.keys())}")
            exit(1)
        log_with_timestamp(
            f"ğŸš€ å·²é€‰æ‹©ä»»åŠ¡: '{task_name}' - {templates[task_name].get('description')}"
        )
        return templates[task_name]

    def _setup_tokenizers(self):
        self.precise_tokenizer_type = "api"
        self.rough_tokenizer = tiktoken.get_encoding("cl100k_base")
        log_with_timestamp("  - ç²—ç®—å°†ä½¿ç”¨: tiktoken")
        if self.llm_provider_name == "deepseek":
            tokenizer_path = self.provider_config.get("tokenizer_path")
            if tokenizer_path and os.path.isdir(tokenizer_path):
                try:
                    log_with_timestamp(
                        "  - æ­£åœ¨åŠ è½½ DeepSeek æœ¬åœ° Tokenizer ç”¨äºç²¾ç®—..."
                    )
                    self.precise_tokenizer = transformers.AutoTokenizer.from_pretrained(
                        tokenizer_path, trust_remote_code=True
                    )
                    self.precise_tokenizer_type = "local_exact"
                except Exception as e:
                    log_with_timestamp(f"  - âŒ åŠ è½½ DeepSeek Tokenizer å¤±è´¥: {e}ã€‚")
            else:
                log_with_timestamp(f"  - âš ï¸ æœªæ‰¾åˆ° DeepSeek Tokenizerï¼Œç²¾ç®—å°†ä¸å¯ç”¨ã€‚")
        log_with_timestamp(f"  - ç²¾ç®—æ–¹å¼å·²ç¡®å®š: {self.precise_tokenizer_type}")

    def _clean_data(self, records: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        if not records:
            return []
        log_with_timestamp("ğŸ§¹ å¼€å§‹æ•°æ®æ¸…æ´—...")
        high_similarity_threshold = self.config["high_similarity_threshold"]
        min_diff_chars = self.config["min_diff_chars"]
        first_valid_index = next(
            (i for i, r in enumerate(records) if r.get("text")), -1
        )
        if first_valid_index == -1:
            return []
        cleaned_records = [records[first_valid_index]]
        for i in range(first_valid_index + 1, len(records)):
            current_record, last_kept_record = records[i], cleaned_records[-1]
            text_current, text_last_kept = current_record.get(
                "text", ""
            ), last_kept_record.get("text", "")
            if not text_current:
                continue
            embeddings = self.similarity_model.encode([text_last_kept, text_current])
            if (
                util.cos_sim(embeddings[0], embeddings[1]).item()
                > high_similarity_threshold
            ):
                continue
            diff = difflib.ndiff(
                text_last_kept.splitlines(keepends=True),
                text_current.splitlines(keepends=True),
            )
            if (
                sum(len(line[2:]) for line in diff if line.startswith("+ "))
                < min_diff_chars
            ):
                continue
            cleaned_records.append(current_record)
        log_with_timestamp(
            f"âœ… æ¸…æ´—å®Œæˆã€‚è®°å½•ä» {len(records)} æ¡å‡å°‘åˆ° {len(cleaned_records)} æ¡ã€‚"
        )
        return cleaned_records

    def _estimate_precise_tokens(self, text: str) -> int:
        if self.precise_tokenizer_type == "api":  # Gemini
            return self.llm_connector.client.count_tokens(text).total_tokens
        elif self.precise_tokenizer_type == "local_exact":  # DeepSeek
            return len(self.precise_tokenizer.encode(text))
        return 0

    def _estimate_rough_tokens(self, text: str) -> int:
        return len(self.rough_tokenizer.encode(text))

    def _chunk_data_efficiently(
        self, records: List[Dict[str, Any]], max_chunk_tokens: int
    ) -> List[str]:
        """
        *** å…³é”®å˜æ›´: æœ€ç»ˆçš„â€œé¢„è®¡ç®—+æŒ‡é’ˆç´¯åŠ â€åˆ†æ®µç®—æ³• ***
        """
        log_with_timestamp("â³ æ­£åœ¨é«˜æ•ˆåœ°å¯¹æ•°æ®è¿›è¡Œåˆ†æ®µ...")

        # 1. é¢„è®¡ç®—é˜¶æ®µ
        log_with_timestamp("  - æ­¥éª¤1: æ­£åœ¨å¯¹æ‰€æœ‰è®°å½•è¿›è¡ŒTokenç²—ç®—é¢„è®¡ç®—...")
        records_with_rough_tokens = [
            (
                record,
                self._estimate_rough_tokens(
                    f"Timestamp: {record['captured_at']}\n{record['text']}"
                ),
            )
            for record in records
        ]
        log_with_timestamp("  - âœ… é¢„è®¡ç®—å®Œæˆã€‚")

        chunks = []
        start_index = 0
        while start_index < len(records_with_rough_tokens):
            # 2. æŒ‡é’ˆç´¯åŠ æ‰“åŒ…é˜¶æ®µ
            current_rough_tokens = 0
            end_index = start_index
            while end_index < len(records_with_rough_tokens):
                current_rough_tokens += records_with_rough_tokens[end_index][1]
                if current_rough_tokens > max_chunk_tokens and end_index > start_index:
                    # ç²—ç®—è¶…é™ï¼Œå›é€€ä¸€æ­¥ï¼Œæ‰¾åˆ°äº†ä¸€ä¸ªç²—ç•¥çš„å—
                    current_rough_tokens -= records_with_rough_tokens[end_index][1]
                    break
                end_index += 1

            # æå–ç²—ç•¥å—çš„è®°å½•
            current_chunk_records = [
                rec for rec, token in records_with_rough_tokens[start_index:end_index]
            ]
            temp_chunk_text = "\n\n---\n\n".join(
                [
                    f"Timestamp: {r['captured_at']}\n{r['text']}"
                    for r in current_chunk_records
                ]
            )

            # 3. ç²¾ç¡®ä¿®æ­£é˜¶æ®µ
            log_with_timestamp(
                f"  - ç²—ç®—æ‰“åŒ…å®Œæˆä¸€ä¸ªå— (è®°å½• {start_index}-{end_index-1}ï¼Œç²—ç®— {current_rough_tokens})ï¼Œå¼€å§‹ç²¾ç®—..."
            )
            precise_tokens = self._estimate_precise_tokens(temp_chunk_text)
            log_with_timestamp(f"  - ç²¾ç®—ç»“æœ: {precise_tokens} tokensã€‚")

            while precise_tokens > max_chunk_tokens and len(current_chunk_records) > 1:
                log_with_timestamp(
                    f"  - ç²¾ç®—åä»è¶…é™ (æº¢å‡º {precise_tokens - max_chunk_tokens} tokens)ï¼Œå¼€å§‹åŠ¨æ€ç§»é™¤..."
                )
                token_overflow = precise_tokens - max_chunk_tokens
                avg_tokens_per_record = precise_tokens / len(current_chunk_records)
                num_to_remove = (
                    math.ceil(token_overflow / avg_tokens_per_record)
                    if avg_tokens_per_record > 0
                    else 1
                )
                num_to_remove = max(1, num_to_remove)

                log_with_timestamp(
                    f"  - åŠ¨æ€è®¡ç®—ï¼šé¢„ä¼°éœ€ç§»é™¤ {num_to_remove} æ¡è®°å½•..."
                )

                current_chunk_records = current_chunk_records[:-num_to_remove]
                end_index -= num_to_remove

                temp_chunk_text = "\n\n---\n\n".join(
                    [
                        f"Timestamp: {r['captured_at']}\n{r['text']}"
                        for r in current_chunk_records
                    ]
                )
                precise_tokens = self._estimate_precise_tokens(temp_chunk_text)
                log_with_timestamp(f"  - ä¿®æ­£åç²¾ç®—ç»“æœ: {precise_tokens} tokensã€‚")

            chunks.append(temp_chunk_text)
            start_index = end_index

        log_with_timestamp(f"âœ… åˆ†æ®µå®Œæˆã€‚æ•°æ®è¢«åˆ†ä¸º {len(chunks)} ä¸ªæ®µè½ã€‚")
        return chunks

    def run(self, start_time: datetime, end_time: datetime):
        raw_records = self.data_fetcher.fetch_data(start_time, end_time)
        cleaned_records = self._clean_data(raw_records)
        if not cleaned_records:
            log_with_timestamp("â„¹ï¸ æ¸…æ´—åæ— æœ‰æ•ˆæ•°æ®ï¼Œæ— æ³•ç”ŸæˆæŠ¥å‘Šã€‚")
            return

        log_with_timestamp("ğŸ“Š æ­£åœ¨ä½¿ç”¨ç²¾ç¡®æ–¹æ³•è®¡ç®—æ€» Token æ•°ï¼Œè¯·ç¨å€™...")
        full_text = "\n\n---\n\n".join(
            [f"Timestamp: {r['captured_at']}\n{r['text']}" for r in cleaned_records]
        )
        total_tokens = self._estimate_precise_tokens(full_text)
        log_with_timestamp(f"ğŸ“Š ç²¾ç¡®æ€» Token æ•°è®¡ç®—å®Œæˆ: {total_tokens}")

        llm_context = ""
        llm_context_window = self.provider_config["context_window"]
        effective_direct_summary_threshold = min(
            self.config["direct_summary_threshold"], llm_context_window
        )

        if total_tokens < effective_direct_summary_threshold:
            log_with_timestamp("ğŸ“ˆ Token æ€»æ•°å°äºé˜ˆå€¼ï¼Œç›´æ¥ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Šã€‚")
            llm_context = full_text
        else:
            log_with_timestamp("ğŸ“‰ Token æ€»æ•°è¶…è¿‡é˜ˆå€¼ï¼Œæ‰§è¡Œåˆ†æ®µæ‘˜è¦æµç¨‹ã€‚")
            max_chunk_tokens = (
                self.provider_config["context_window"] - self.config["token_headroom"]
            )
            log_with_timestamp(f"  - æ¯ä¸ªåˆ†æ®µæœ€å¤§ Token ä¸Šé™: {max_chunk_tokens}")

            chunks = self._chunk_data_efficiently(cleaned_records, max_chunk_tokens)

            chunk_prompt_template = self.task_template["chunk_summary_prompt"]
            system_prompt = self.task_template.get("system_prompt")
            summaries = []
            api_delay = self.provider_config.get("api_call_delay_seconds", 0)
            for i, chunk in enumerate(chunks):
                summary = self.llm_connector.generate(
                    user_prompt=chunk_prompt_template.format(chunk_text=chunk),
                    system_prompt=system_prompt,
                    temperature=self.temperature,
                )
                summaries.append(summary)
                if i < len(chunks) - 1 and api_delay > 0:
                    log_with_timestamp(
                        f"â¸ï¸ æ£€æµ‹åˆ° {self.llm_provider_name} éœ€è¦å»¶è¿Ÿè°ƒç”¨ï¼Œæš‚åœ {api_delay} ç§’..."
                    )
                    time.sleep(api_delay)

            llm_context = "\n\n".join(summaries)

        log_with_timestamp("ğŸ–‹ï¸ å¼€å§‹ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š...")
        final_prompt_template = self.task_template["final_report_prompt"]
        final_report = self.llm_connector.generate(
            user_prompt=final_prompt_template.format(all_summaries=llm_context),
            system_prompt=system_prompt,
            temperature=self.temperature,
        )

        output_dir = self.config["output_path"]
        os.makedirs(output_dir, exist_ok=True)
        timestamp_str = datetime.now().strftime("%Y-%m-%d_%H%M%S")
        report_filename = (
            f"{timestamp_str}_{self.cli_args.task}_{self.llm_provider_name}.md"
        )
        report_path = os.path.join(output_dir, report_filename)
        with open(report_path, "w", encoding="utf-8") as f:
            f.write(final_report)
        log_with_timestamp(f"\nğŸ‰ æˆåŠŸï¼æŠ¥å‘Šå·²ä¿å­˜è‡³: {report_path}")


def load_config(path: str) -> Dict[str, Any]:
    try:
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)
    except FileNotFoundError:
        log_with_timestamp(f"é”™è¯¯: é…ç½®æ–‡ä»¶ '{path}' æœªæ‰¾åˆ°ã€‚")
        exit(1)
    except json.JSONDecodeError:
        log_with_timestamp(f"é”™è¯¯: é…ç½®æ–‡ä»¶ '{path}' æ ¼å¼ä¸æ­£ç¡®ã€‚")
        exit(1)


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Screenpipe OCR æ™ºèƒ½æ—¥æŠ¥ç”ŸæˆåŠ©æ‰‹",
        formatter_class=argparse.RawTextHelpFormatter,
    )
    parser.add_argument(
        "--start_time",
        type=str,
        help="å¼€å§‹æ—¶é—´ (æ ¼å¼: YYYY-MM-DDTHH:MM:SS)ï¼Œé»˜è®¤ä¸º24å°æ—¶å‰",
        default=(datetime.now() - timedelta(days=1)).isoformat(timespec="seconds"),
    )
    parser.add_argument(
        "--end_time",
        type=str,
        help="ç»“æŸæ—¶é—´ (æ ¼å¼: YYYY-MM-DDTHH:MM:SS)ï¼Œé»˜è®¤ä¸ºå½“å‰æ—¶é—´",
        default=datetime.now().isoformat(timespec="seconds"),
    )
    parser.add_argument(
        "--config", type=str, default="config.json", help="é…ç½®æ–‡ä»¶çš„è·¯å¾„"
    )
    parser.add_argument(
        "--llm",
        type=str,
        help="é€‰æ‹©ä½¿ç”¨çš„å¤§è¯­è¨€æ¨¡å‹ (ä¾‹å¦‚: gemini, deepseek)ã€‚ä¼šè¦†ç›–é…ç½®æ–‡ä»¶ä¸­çš„è®¾ç½®ã€‚",
    )
    parser.add_argument(
        "--task",
        type=str,
        default="daily_report",
        help="é€‰æ‹©è¦æ‰§è¡Œçš„ä»»åŠ¡ (å¯¹åº” config.json ä¸­çš„ prompt_templates key)ã€‚\nä¾‹å¦‚: daily_report, tutorial_generator",
    )
    parser.add_argument(
        "--temperature",
        type=float,
        help="è®¾ç½® LLM çš„ Temperatureã€‚ä¼šè¦†ç›–é…ç½®æ–‡ä»¶ä¸­çš„è®¾ç½®ã€‚",
    )
    args = parser.parse_args()

    config = load_config(args.config)
    llm_provider = args.llm or config.get("llm_provider")
    if llm_provider == "gemini":
        gemini_config = config.get("llm_config", {}).get("gemini", {})
        proxy = gemini_config.get("proxy")
        if proxy:
            os.environ["HTTP_PROXY"] = proxy
            os.environ["HTTPS_PROXY"] = proxy
            log_with_timestamp(f"ğŸ”§ å…¨å±€ä»£ç†å·²ä¸º Gemini è®¾ç½®: {proxy}")
    try:
        start_dt, end_dt = datetime.fromisoformat(
            args.start_time
        ), datetime.fromisoformat(args.end_time)
    except ValueError:
        log_with_timestamp("é”™è¯¯: æ—¶é—´æ ¼å¼ä¸æ­£ç¡®ã€‚è¯·ä½¿ç”¨ 'YYYY-MM-DDTHH:MM:SS' æ ¼å¼ã€‚")
        exit(1)
    generator = DailyReportGenerator(config=config, cli_args=args)
    generator.run(start_time=start_dt, end_time=end_dt)
