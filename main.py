# main.py
import os
import json
import argparse
import difflib
import sqlite3
import platform
import time
import math
from pathlib import Path
from datetime import datetime, timedelta, timezone
from typing import List, Dict, Any, Optional

# --- åŠ¨æ€å¯¼å…¥æ£€æŸ¥ ---
try:
    import tiktoken
    from sentence_transformers import SentenceTransformer, util
    import google.generativeai as genai
    from openai import OpenAI
    import transformers
    from PIL import Image
except ImportError as e:
    print(f"ä¾èµ–åº“å¯¼å…¥å¤±è´¥: {e}")
    print("è¯·ç¡®ä¿å·²é€šè¿‡ 'pip install -r requirements.txt' å®‰è£…æ‰€æœ‰ä¾èµ–ã€‚")
    exit(1)


# --- å¸¦æ—¶é—´æˆ³çš„æ—¥å¿—å‡½æ•° ---
def log_with_timestamp(message: str):
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    print(f"[{timestamp}] {message}")


# *** å…³é”®å˜æ›´: å¼•å…¥é¢å‘å¯¹è±¡çš„ LLM è¿æ¥å™¨æ¶æ„ ***


class BaseLLMConnector:
    """æ‰€æœ‰ LLM è¿æ¥å™¨çš„åŸºç±»ï¼Œå®šä¹‰äº†ç»Ÿä¸€çš„æ¥å£ã€‚"""

    def __init__(self, provider_config: Dict[str, Any]):
        self.provider_config = provider_config
        self.model_name = provider_config.get("model_name")
        self.client = self._initialize_client()
        log_with_timestamp(
            f"ğŸ¤– {self.__class__.__name__} å·²åˆå§‹åŒ– (æ¨¡å‹: {self.model_name})"
        )

    def _initialize_client(self):
        raise NotImplementedError

    def generate(
        self,
        user_prompt: str,
        system_prompt: Optional[str] = None,
        temperature: float = 0.7,
        attachment_data: Optional[Any] = None,
        attachment_type: Optional[str] = None,
    ) -> str:
        raise NotImplementedError

    def count_tokens(self, text: str) -> int:
        raise NotImplementedError


class GeminiConnector(BaseLLMConnector):
    """å¤„ç† Google Gemini æ¨¡å‹çš„è¿æ¥å™¨ã€‚"""

    def _initialize_client(self):
        genai.configure(api_key=self.provider_config["api_key"])
        return genai.GenerativeModel(self.model_name)

    def generate(
        self,
        user_prompt: str,
        system_prompt: Optional[str] = None,
        temperature: float = 0.7,
        attachment_data: Optional[Any] = None,
        attachment_type: Optional[str] = None,
    ) -> str:
        log_with_timestamp(f"ğŸš€ å‘èµ· Gemini API è°ƒç”¨ (Temperature: {temperature})...")
        try:
            generation_config = genai.types.GenerationConfig(temperature=temperature)
            content = [user_prompt]
            if attachment_data:
                if attachment_type == "image":
                    log_with_timestamp("  - æ­£åœ¨å°†å›¾ç‰‡é™„ä»¶æ·»åŠ åˆ° Gemini è¯·æ±‚ä¸­...")
                    content.append(attachment_data)
                elif attachment_type == "text":
                    content[0] = (
                        f"### è¡¥å……ä¸Šä¸‹æ–‡:\n{attachment_data}\n\n### ä¸»è¦ä»»åŠ¡:\n{user_prompt}"
                    )
            if system_prompt:
                content[0] = f"{system_prompt}\n\n---\n\n{content[0]}"
            response = self.client.generate_content(
                content, generation_config=generation_config
            )
            return response.text
        except Exception as e:
            log_with_timestamp(f"âŒ Gemini API è°ƒç”¨å¤±è´¥: {e}")
            return f"[LLM è°ƒç”¨é”™è¯¯: {e}]"
        return "[LLM è°ƒç”¨è¿”å›ç©º]"

    def count_tokens(self, text: str) -> int:
        return self.client.count_tokens(text).total_tokens


class DeepSeekConnector(BaseLLMConnector):
    """å¤„ç† DeepSeek å’Œå…¶ä»– OpenAI å…¼å®¹ API çš„è¿æ¥å™¨ã€‚"""

    def _initialize_client(self):
        return OpenAI(
            api_key=self.provider_config["api_key"],
            base_url=self.provider_config["base_url"],
        )

    def generate(
        self,
        user_prompt: str,
        system_prompt: Optional[str] = None,
        temperature: float = 0.7,
        attachment_data: Optional[Any] = None,
        attachment_type: Optional[str] = None,
    ) -> str:
        log_with_timestamp(f"ğŸš€ å‘èµ· DeepSeek API è°ƒç”¨ (Temperature: {temperature})...")

        # å½“å‰çš„ä¿æŠ¤æ€§æªæ–½
        if attachment_type == "image":
            log_with_timestamp(
                "âŒ é”™è¯¯: DeepSeek çš„ OpenAI å…¼å®¹ API å½“å‰ä¸æ”¯æŒç›´æ¥çš„å›¾åƒè¾“å…¥ã€‚"
            )
            log_with_timestamp(
                "   (æ¨¡å‹æœ¬èº«å…·å¤‡å¤šæ¨¡æ€èƒ½åŠ›ï¼Œä½†éœ€è¦ç­‰å¾…å®˜æ–¹æ›´æ–°å…¶å…¬å…±APIä»¥æ”¯æŒæ­¤åŠŸèƒ½)"
            )
            return "[é”™è¯¯: æ­¤æ¨¡å‹ API ä¸æ”¯æŒå›¾åƒè¾“å…¥]"

        try:
            messages = []
            if system_prompt:
                messages.append({"role": "system", "content": system_prompt})
            if attachment_type == "text" and attachment_data:
                user_prompt = f"### è¡¥å……ä¸Šä¸‹æ–‡:\n{attachment_data}\n\n### ä¸»è¦ä»»åŠ¡:\n{user_prompt}"
            messages.append({"role": "user", "content": user_prompt})
            chat_completion = self.client.chat.completions.create(
                messages=messages, model=self.model_name, temperature=temperature
            )
            return chat_completion.choices[0].message.content
        except Exception as e:
            log_with_timestamp(f"âŒ DeepSeek API è°ƒç”¨å¤±è´¥: {e}")
            return f"[LLM è°ƒç”¨é”™è¯¯: {e}]"
        return "[LLM è°ƒç”¨è¿”å›ç©º]"

    def count_tokens(self, text: str) -> int:
        # DeepSeek çš„ Token è®¡ç®—ç”± DailyReportGenerator ä¸­çš„æœ¬åœ° Tokenizer å¤„ç†
        # è¿™ä¸ªæ–¹æ³•åœ¨è¿™é‡Œåªæ˜¯ä¸ºäº†æ»¡è¶³æ¥å£ç»Ÿä¸€æ€§ï¼Œå®é™…ä¸Šä¸ä¼šè¢«è°ƒç”¨
        raise NotImplementedError("DeepSeek token counting should be handled locally.")


class LLMConnectorFactory:
    """æ ¹æ®æä¾›å•†åç§°åˆ›å»ºç›¸åº”çš„è¿æ¥å™¨å®ä¾‹ã€‚"""

    @staticmethod
    def create(provider_name: str, provider_config: Dict[str, Any]) -> BaseLLMConnector:
        if provider_name == "gemini":
            return GeminiConnector(provider_config)
        elif provider_name == "deepseek":
            return DeepSeekConnector(provider_config)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„ LLM provider: {provider_name}")


# --- æ¨¡å— 1.1: æ•°æ®è·å–æ¨¡å— ---
class DataFetcher:
    # ... æ­¤éƒ¨åˆ†ä»£ç æ— å˜åŒ– ...
    def __init__(self):
        self.db_path = self._get_db_path()
        if not self.db_path or not self.db_path.exists():
            log_with_timestamp(
                f"âŒ é”™è¯¯: æœªèƒ½æ‰¾åˆ° Screenpipe æ•°æ®åº“ã€‚é¢„æœŸè·¯å¾„: {self.db_path}"
            )
            exit(1)
        log_with_timestamp(f"ğŸ” æˆåŠŸå®šä½ Screenpipe æ•°æ®åº“: {self.db_path}")

    def _get_db_path(self) -> Path | None:
        system = platform.system()
        home = Path.home()
        if system == "Windows":
            path1 = home / ".screenpipe/db.sqlite"
            path2 = home / "AppData/Roaming/Screenpipe/db.sqlite"
            if path1.exists():
                return path1
            return path2
        if system == "Darwin":
            return home / "Library/Application Support/Screenpipe/db.sqlite"
        if system == "Linux":
            return home / ".config/Screenpipe/db.sqlite"
        return None

    def fetch_data(
        self, start_time: datetime, end_time: datetime
    ) -> List[Dict[str, Any]]:
        log_with_timestamp(
            f"ğŸ’¾ æ­£åœ¨ä»æ•°æ®åº“è·å– {start_time.isoformat()} åˆ° {end_time.isoformat()} çš„ OCR æ•°æ®..."
        )
        query = "SELECT ocr.text, frm.timestamp AS captured_at FROM frames AS frm JOIN ocr_text AS ocr ON frm.id = ocr.frame_id WHERE frm.timestamp >= ? AND frm.timestamp <= ? ORDER BY frm.timestamp ASC;"
        records = []
        try:
            with sqlite3.connect(f"file:{self.db_path}?mode=ro", uri=True) as conn:
                conn.row_factory = sqlite3.Row
                cursor = conn.cursor()
                cursor.execute(query, (start_time.isoformat(), end_time.isoformat()))
                for row in cursor.fetchall():
                    records.append(dict(row))
            log_with_timestamp(f"âœ… æˆåŠŸè·å– {len(records)} æ¡ OCR è®°å½•ã€‚")
            return records
        except sqlite3.Error as e:
            log_with_timestamp(f"âŒ æ•°æ®åº“æŸ¥è¯¢å¤±è´¥: {e}")
            return []


# --- æ ¸å¿ƒåº”ç”¨ç±» ---
class DailyReportGenerator:
    def __init__(self, config: Dict[str, Any], cli_args: argparse.Namespace):
        self.config = config
        self.data_fetcher = DataFetcher()
        self.cli_args = cli_args
        self.llm_provider_name = cli_args.llm or self.config["llm_provider"]
        log_with_timestamp(f"ğŸ”§ LLM æä¾›å•†å·²ç¡®å®š: {self.llm_provider_name}")
        self.provider_config = self.config["llm_config"][self.llm_provider_name]

        # *** å…³é”®å˜æ›´: ä½¿ç”¨å·¥å‚æ¨¡å¼åˆ›å»ºè¿æ¥å™¨ ***
        self.llm_connector = LLMConnectorFactory.create(
            provider_name=self.llm_provider_name, provider_config=self.provider_config
        )

        self.task_template = self._load_task_template(cli_args.task)
        if cli_args.temperature is not None:
            self.temperature = cli_args.temperature
            log_with_timestamp(
                f"ğŸ”§ å·²é€šè¿‡å‘½ä»¤è¡Œå‚æ•°è®¾ç½® Temperature: {self.temperature}"
            )
        else:
            self.temperature = self.provider_config.get("temperature", 0.7)
        log_with_timestamp("æ­£åœ¨åŠ è½½ NLP æ¨¡å‹...")
        self.similarity_model = SentenceTransformer("all-MiniLM-L6-v2")
        self._setup_tokenizers()
        log_with_timestamp("æ¨¡å‹åŠ è½½å®Œæˆã€‚")

    def _load_task_template(self, task_name: str) -> Dict[str, str]:
        templates = self.config.get("prompt_templates", {})
        if task_name not in templates:
            log_with_timestamp(f"âŒ é”™è¯¯: ä»»åŠ¡ '{task_name}' åœ¨é…ç½®æ–‡ä»¶ä¸­æœªå®šä¹‰ã€‚")
            log_with_timestamp(f"å¯ç”¨çš„ä»»åŠ¡æœ‰: {list(templates.keys())}")
            exit(1)
        log_with_timestamp(
            f"ğŸš€ å·²é€‰æ‹©ä»»åŠ¡: '{task_name}' - {templates[task_name].get('description')}"
        )
        return templates[task_name]

    def _setup_tokenizers(self):
        self.precise_tokenizer = None
        self.rough_tokenizer = tiktoken.get_encoding("cl100k_base")
        log_with_timestamp("  - ç²—ç®—å°†ä½¿ç”¨: tiktoken")
        if self.llm_provider_name == "deepseek":
            tokenizer_path = self.provider_config.get("tokenizer_path")
            if tokenizer_path and os.path.isdir(tokenizer_path):
                try:
                    log_with_timestamp(
                        "  - æ­£åœ¨åŠ è½½ DeepSeek æœ¬åœ° Tokenizer ç”¨äºç²¾ç®—..."
                    )
                    self.precise_tokenizer = transformers.AutoTokenizer.from_pretrained(
                        tokenizer_path, trust_remote_code=True
                    )
                except Exception as e:
                    log_with_timestamp(f"  - âŒ åŠ è½½ DeepSeek Tokenizer å¤±è´¥: {e}ã€‚")
        log_with_timestamp(
            f"  - ç²¾ç®—æ–¹å¼å·²ç¡®å®š: {'local_exact' if self.precise_tokenizer else 'api'}"
        )

    def _clean_data(self, records: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        if not records:
            return []
        log_with_timestamp("ğŸ§¹ å¼€å§‹æ•°æ®æ¸…æ´—...")
        high_similarity_threshold = self.config["high_similarity_threshold"]
        min_diff_chars = self.config["min_diff_chars"]
        first_valid_index = next(
            (i for i, r in enumerate(records) if r.get("text")), -1
        )
        if first_valid_index == -1:
            return []
        cleaned_records = [records[first_valid_index]]
        for i in range(first_valid_index + 1, len(records)):
            current_record, last_kept_record = records[i], cleaned_records[-1]
            text_current, text_last_kept = current_record.get(
                "text", ""
            ), last_kept_record.get("text", "")
            if not text_current:
                continue
            embeddings = self.similarity_model.encode([text_last_kept, text_current])
            if (
                util.cos_sim(embeddings[0], embeddings[1]).item()
                > high_similarity_threshold
            ):
                continue
            diff = difflib.ndiff(
                text_last_kept.splitlines(keepends=True),
                text_current.splitlines(keepends=True),
            )
            if (
                sum(len(line[2:]) for line in diff if line.startswith("+ "))
                < min_diff_chars
            ):
                continue
            cleaned_records.append(current_record)
        log_with_timestamp(
            f"âœ… æ¸…æ´—å®Œæˆã€‚è®°å½•ä» {len(records)} æ¡å‡å°‘åˆ° {len(cleaned_records)} æ¡ã€‚"
        )
        return cleaned_records

    def _estimate_precise_tokens(self, text: str) -> int:
        if self.llm_provider_name == "gemini":
            return self.llm_connector.count_tokens(text)
        elif self.llm_provider_name == "deepseek" and self.precise_tokenizer:
            return len(self.precise_tokenizer.encode(text))
        # å¦‚æœæ²¡æœ‰ç²¾ç¡®æ–¹æ³•ï¼Œå›é€€åˆ°ç²—ç®—
        return self._estimate_rough_tokens(text)

    def _estimate_rough_tokens(self, text: str) -> int:
        return len(self.rough_tokenizer.encode(text))

    def _chunk_data_efficiently(
        self, records: List[Dict[str, Any]], max_chunk_tokens: int
    ) -> List[str]:
        log_with_timestamp("â³ æ­£åœ¨é«˜æ•ˆåœ°å¯¹æ•°æ®è¿›è¡Œåˆ†æ®µ...")
        log_with_timestamp("  - æ­¥éª¤1: æ­£åœ¨å¯¹æ‰€æœ‰è®°å½•è¿›è¡ŒTokenç²—ç®—é¢„è®¡ç®—...")
        records_with_rough_tokens = [
            (
                record,
                self._estimate_rough_tokens(
                    f"Timestamp: {record['captured_at']}\n{record['text']}"
                ),
            )
            for record in records
        ]
        log_with_timestamp("  - âœ… é¢„è®¡ç®—å®Œæˆã€‚")
        chunks = []
        start_index = 0
        while start_index < len(records_with_rough_tokens):
            current_rough_tokens = 0
            end_index = start_index
            while end_index < len(records_with_rough_tokens):
                current_rough_tokens += records_with_rough_tokens[end_index][1]
                if current_rough_tokens > max_chunk_tokens and end_index > start_index:
                    current_rough_tokens -= records_with_rough_tokens[end_index][1]
                    break
                end_index += 1
            current_chunk_records = [
                rec for rec, token in records_with_rough_tokens[start_index:end_index]
            ]
            temp_chunk_text = "\n\n---\n\n".join(
                [
                    f"Timestamp: {r['captured_at']}\n{r['text']}"
                    for r in current_chunk_records
                ]
            )
            log_with_timestamp(
                f"  - ç²—ç®—æ‰“åŒ…å®Œæˆä¸€ä¸ªå— (è®°å½• {start_index}-{end_index-1}ï¼Œç²—ç®— {current_rough_tokens})ï¼Œå¼€å§‹ç²¾ç®—..."
            )
            precise_tokens = self._estimate_precise_tokens(temp_chunk_text)
            log_with_timestamp(f"  - ç²¾ç®—ç»“æœ: {precise_tokens} tokensã€‚")
            while precise_tokens > max_chunk_tokens and len(current_chunk_records) > 1:
                log_with_timestamp(
                    f"  - ç²¾ç®—åä»è¶…é™ (æº¢å‡º {precise_tokens - max_chunk_tokens} tokens)ï¼Œå¼€å§‹åŠ¨æ€ç§»é™¤..."
                )
                token_overflow = precise_tokens - max_chunk_tokens
                avg_tokens_per_record = precise_tokens / len(current_chunk_records)
                num_to_remove = (
                    math.ceil(token_overflow / avg_tokens_per_record)
                    if avg_tokens_per_record > 0
                    else 1
                )
                num_to_remove = max(1, num_to_remove)
                log_with_timestamp(
                    f"  - åŠ¨æ€è®¡ç®—ï¼šé¢„ä¼°éœ€ç§»é™¤ {num_to_remove} æ¡è®°å½•..."
                )
                current_chunk_records = current_chunk_records[:-num_to_remove]
                end_index -= num_to_remove
                temp_chunk_text = "\n\n---\n\n".join(
                    [
                        f"Timestamp: {r['captured_at']}\n{r['text']}"
                        for r in current_chunk_records
                    ]
                )
                precise_tokens = self._estimate_precise_tokens(temp_chunk_text)
                log_with_timestamp(f"  - ä¿®æ­£åç²¾ç®—ç»“æœ: {precise_tokens} tokensã€‚")
            chunks.append(temp_chunk_text)
            start_index = end_index
        log_with_timestamp(f"âœ… åˆ†æ®µå®Œæˆã€‚æ•°æ®è¢«åˆ†ä¸º {len(chunks)} ä¸ªæ®µè½ã€‚")
        return chunks

    def _generate_final_report(
        self,
        llm_context: str,
        run_output_dir: Path,
        attachment_data: Optional[Any] = None,
        attachment_type: Optional[str] = None,
    ):
        log_with_timestamp("ğŸ–‹ï¸ å¼€å§‹ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š...")
        final_prompt_template = self.task_template["final_report_prompt"]
        final_prompt = final_prompt_template.format(all_summaries=llm_context)
        final_report = self.llm_connector.generate(
            user_prompt=final_prompt,
            system_prompt=self.task_template.get("system_prompt"),
            temperature=self.temperature,
            attachment_data=attachment_data,
            attachment_type=attachment_type,
        )
        report_path = run_output_dir / "final_report.md"
        with open(report_path, "w", encoding="utf-8") as f:
            f.write(final_report)
        log_with_timestamp(f"\nğŸ‰ æˆåŠŸï¼æŠ¥å‘Šå·²ä¿å­˜è‡³: {report_path}")

    def run(
        self,
        start_time: datetime,
        end_time: datetime,
        attachment_data: Optional[Any] = None,
        attachment_type: Optional[str] = None,
    ):
        run_output_dir = (
            Path(self.config["output_path"])
            / f"{datetime.now().strftime('%Y-%m-%d_%H%M%S')}_{self.cli_args.task}_{self.llm_provider_name}"
        )
        run_output_dir.mkdir(parents=True, exist_ok=True)
        log_with_timestamp(f"ğŸ“‚ æœ¬æ¬¡è¿è¡Œä¼šè¯ç›®å½•å·²åˆ›å»º: {run_output_dir}")
        raw_records = self.data_fetcher.fetch_data(start_time, end_time)
        cleaned_records = self._clean_data(raw_records)
        if not cleaned_records:
            log_with_timestamp("â„¹ï¸ æ¸…æ´—åæ— æœ‰æ•ˆæ•°æ®ï¼Œæ— æ³•ç”ŸæˆæŠ¥å‘Šã€‚")
            return
        log_with_timestamp("ğŸ“Š æ­£åœ¨ä½¿ç”¨ç²¾ç¡®æ–¹æ³•è®¡ç®—æ€» Token æ•°ï¼Œè¯·ç¨å€™...")
        full_text = "\n\n---\n\n".join(
            [f"Timestamp: {r['captured_at']}\n{r['text']}" for r in cleaned_records]
        )
        total_tokens = self._estimate_precise_tokens(full_text)
        log_with_timestamp(f"ğŸ“Š ç²¾ç¡®æ€» Token æ•°è®¡ç®—å®Œæˆ: {total_tokens}")
        llm_context = ""
        llm_context_window = self.provider_config["context_window"]
        effective_direct_summary_threshold = min(
            self.config["direct_summary_threshold"], llm_context_window
        )
        if total_tokens < effective_direct_summary_threshold:
            log_with_timestamp("ğŸ“ˆ Token æ€»æ•°å°äºé˜ˆå€¼ï¼Œç›´æ¥ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Šã€‚")
            llm_context = full_text
        else:
            log_with_timestamp("ğŸ“‰ Token æ€»æ•°è¶…è¿‡é˜ˆå€¼ï¼Œæ‰§è¡Œåˆ†æ®µæ‘˜è¦æµç¨‹ã€‚")
            max_chunk_tokens = (
                self.provider_config["context_window"] - self.config["token_headroom"]
            )
            log_with_timestamp(f"  - æ¯ä¸ªåˆ†æ®µæœ€å¤§ Token ä¸Šé™: {max_chunk_tokens}")
            chunks = self._chunk_data_efficiently(cleaned_records, max_chunk_tokens)
            summaries_dir = run_output_dir / "summaries"
            summaries_dir.mkdir(exist_ok=True)
            log_with_timestamp(f"  - åˆ†æ®µæ‘˜è¦å°†ä¿å­˜è‡³: {summaries_dir}")
            chunk_prompt_template = self.task_template["chunk_summary_prompt"]
            system_prompt = self.task_template.get("system_prompt")
            summaries = []
            api_delay = self.provider_config.get("api_call_delay_seconds", 0)
            for i, chunk in enumerate(chunks):
                summary = self.llm_connector.generate(
                    user_prompt=chunk_prompt_template.format(chunk_text=chunk),
                    system_prompt=system_prompt,
                    temperature=self.temperature,
                )
                summaries.append(summary)
                summary_filename = f"{i+1:02d}_summary.txt"
                summary_path = summaries_dir / summary_filename
                with open(summary_path, "w", encoding="utf-8") as f:
                    f.write(summary)
                log_with_timestamp(f"  - âœ… å·²ä¿å­˜æ‘˜è¦ {summary_filename}")
                if i < len(chunks) - 1 and api_delay > 0:
                    log_with_timestamp(
                        f"â¸ï¸ æ£€æµ‹åˆ° {self.llm_provider_name} éœ€è¦å»¶è¿Ÿè°ƒç”¨ï¼Œæš‚åœ {api_delay} ç§’..."
                    )
                    time.sleep(api_delay)
            llm_context = "\n\n".join(summaries)
        self._generate_final_report(
            llm_context, run_output_dir, attachment_data, attachment_type
        )

    def run_from_summaries(
        self,
        summary_dir_path: Path,
        attachment_data: Optional[Any] = None,
        attachment_type: Optional[str] = None,
    ):
        log_with_timestamp(f"ğŸ”„ è¿›å…¥äºŒæ¬¡å¤„ç†æ¨¡å¼ï¼Œä»æ–‡ä»¶å¤¹åŠ è½½æ‘˜è¦: {summary_dir_path}")
        if not summary_dir_path.is_dir():
            log_with_timestamp(f"âŒ é”™è¯¯: æä¾›çš„è·¯å¾„ä¸æ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„æ–‡ä»¶å¤¹ã€‚")
            return
        summary_files = sorted(summary_dir_path.glob("*.txt"))
        if not summary_files:
            log_with_timestamp(
                f"âŒ é”™è¯¯: åœ¨ {summary_dir_path} ä¸­æ²¡æœ‰æ‰¾åˆ°ä»»ä½• .txt æ‘˜è¦æ–‡ä»¶ã€‚"
            )
            return
        log_with_timestamp(f"  - æ‰¾åˆ° {len(summary_files)} ä¸ªæ‘˜è¦æ–‡ä»¶ï¼Œæ­£åœ¨è¯»å–...")
        summaries = []
        for file_path in summary_files:
            with open(file_path, "r", encoding="utf-8") as f:
                summaries.append(f.read())
        llm_context = "\n\n".join(summaries)
        run_output_dir = (
            Path(self.config["output_path"])
            / f"{datetime.now().strftime('%Y-%m-%d_%H%M%S')}_{self.cli_args.task}_{self.llm_provider_name}_finetuned"
        )
        run_output_dir.mkdir(parents=True, exist_ok=True)
        log_with_timestamp(f"ğŸ“‚ æœ¬æ¬¡äºŒæ¬¡å¤„ç†ä¼šè¯ç›®å½•å·²åˆ›å»º: {run_output_dir}")
        self._generate_final_report(
            llm_context, run_output_dir, attachment_data, attachment_type
        )


def load_config(path: str) -> Dict[str, Any]:
    try:
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)
    except FileNotFoundError:
        log_with_timestamp(f"é”™è¯¯: é…ç½®æ–‡ä»¶ '{path}' æœªæ‰¾åˆ°ã€‚")
        exit(1)
    except json.JSONDecodeError:
        log_with_timestamp(f"é”™è¯¯: é…ç½®æ–‡ä»¶ '{path}' æ ¼å¼ä¸æ­£ç¡®ã€‚")
        exit(1)


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Screenpipe OCR æ™ºèƒ½æ—¥æŠ¥ç”ŸæˆåŠ©æ‰‹",
        formatter_class=argparse.RawTextHelpFormatter,
    )
    parser.add_argument(
        "--start_time",
        type=str,
        help="å¼€å§‹æ—¶é—´ (æ ¼å¼: YYYY-MM-DDTHH:MM:SS)ï¼Œé»˜è®¤ä¸º24å°æ—¶å‰",
    )
    parser.add_argument(
        "--end_time",
        type=str,
        help="ç»“æŸæ—¶é—´ (æ ¼å¼: YYYY-MM-DDTHH:MM:SS)ï¼Œé»˜è®¤ä¸ºå½“å‰æ—¶é—´",
    )
    parser.add_argument(
        "--config", type=str, default="config.json", help="é…ç½®æ–‡ä»¶çš„è·¯å¾„"
    )
    parser.add_argument(
        "--llm",
        type=str,
        help="é€‰æ‹©ä½¿ç”¨çš„å¤§è¯­è¨€æ¨¡å‹ (ä¾‹å¦‚: gemini, deepseek)ã€‚ä¼šè¦†ç›–é…ç½®æ–‡ä»¶ä¸­çš„è®¾ç½®ã€‚",
    )
    parser.add_argument(
        "--task",
        type=str,
        default="daily_report",
        help="é€‰æ‹©è¦æ‰§è¡Œçš„ä»»åŠ¡ (å¯¹åº” config.json ä¸­çš„ prompt_templates key)ã€‚\nä¾‹å¦‚: daily_report, tutorial_generator",
    )
    parser.add_argument(
        "--temperature",
        type=float,
        help="è®¾ç½® LLM çš„ Temperatureã€‚ä¼šè¦†ç›–é…ç½®æ–‡ä»¶ä¸­çš„è®¾ç½®ã€‚",
    )
    parser.add_argument(
        "--attachment", type=str, help="æä¾›ä¸€ä¸ªé™„ä»¶çš„è·¯å¾„ (å¯ä»¥æ˜¯æ–‡æœ¬æ–‡ä»¶æˆ–å›¾ç‰‡)ã€‚"
    )
    parser.add_argument(
        "--use_summaries_from",
        type=str,
        help="æä¾›ä¸€ä¸ªåŒ…å«æ‘˜è¦æ–‡ä»¶çš„æ–‡ä»¶å¤¹è·¯å¾„ï¼Œè·³è¿‡æ•°æ®æå–å’Œåˆ†æ®µæ‘˜è¦ï¼Œç›´æ¥ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Šã€‚",
    )
    args = parser.parse_args()

    config = load_config(args.config)
    llm_provider = args.llm or config.get("llm_provider")
    if llm_provider == "gemini":
        gemini_config = config.get("llm_config", {}).get("gemini", {})
        proxy = gemini_config.get("proxy")
        if proxy:
            os.environ["HTTP_PROXY"] = proxy
            os.environ["HTTPS_PROXY"] = proxy
            log_with_timestamp(f"ğŸ”§ å…¨å±€ä»£ç†å·²ä¸º Gemini è®¾ç½®: {proxy}")
    attachment_data = None
    attachment_type = None
    if args.attachment:
        attachment_path = Path(args.attachment)
        if not attachment_path.exists():
            log_with_timestamp(f"âŒ é”™è¯¯: é™„ä»¶æ–‡ä»¶æœªæ‰¾åˆ°: {args.attachment}")
            exit(1)
        suffix = attachment_path.suffix.lower()
        if suffix in [".txt", ".md", ".json", ".xml", ".py", ".js"]:
            try:
                with open(attachment_path, "r", encoding="utf-8") as f:
                    attachment_data = f.read()
                attachment_type = "text"
                log_with_timestamp(f"ğŸ“„ å·²æˆåŠŸåŠ è½½æ–‡æœ¬é™„ä»¶: {args.attachment}")
            except Exception as e:
                log_with_timestamp(f"âŒ é”™è¯¯: è¯»å–æ–‡æœ¬é™„ä»¶å¤±è´¥: {e}")
                exit(1)
        elif suffix in [".png", ".jpg", ".jpeg", ".webp"]:
            try:
                attachment_data = Image.open(attachment_path)
                attachment_type = "image"
                log_with_timestamp(f"ğŸ–¼ï¸ å·²æˆåŠŸåŠ è½½å›¾ç‰‡é™„ä»¶: {args.attachment}")
            except Exception as e:
                log_with_timestamp(f"âŒ é”™è¯¯: è¯»å–å›¾ç‰‡é™„ä»¶å¤±è´¥: {e}")
                exit(1)
        else:
            log_with_timestamp(
                f"âš ï¸ è­¦å‘Š: ä¸æ”¯æŒçš„é™„ä»¶æ–‡ä»¶ç±»å‹ '{suffix}'ã€‚é™„ä»¶å°†è¢«å¿½ç•¥ã€‚"
            )

    generator = DailyReportGenerator(config=config, cli_args=args)

    if args.use_summaries_from:
        summary_dir = Path(args.use_summaries_from)
        generator.run_from_summaries(summary_dir, attachment_data, attachment_type)
    else:
        try:
            if args.start_time:
                start_dt_local = datetime.fromisoformat(args.start_time)
            else:
                start_dt_local = datetime.now() - timedelta(days=1)
            if args.end_time:
                end_dt_local = datetime.fromisoformat(args.end_time)
            else:
                end_dt_local = datetime.now()
            start_dt_utc = start_dt_local.astimezone(timezone.utc)
            end_dt_utc = end_dt_local.astimezone(timezone.utc)
            log_with_timestamp(
                f"æŸ¥è¯¢æ—¶é—´èŒƒå›´ (æœ¬åœ°): {start_dt_local.isoformat(timespec='seconds')} -> {end_dt_local.isoformat(timespec='seconds')}"
            )
            log_with_timestamp(
                f"æŸ¥è¯¢æ—¶é—´èŒƒå›´ (UTC): {start_dt_utc.isoformat(timespec='seconds')} -> {end_dt_utc.isoformat(timespec='seconds')}"
            )
        except ValueError:
            log_with_timestamp(
                "é”™è¯¯: æ—¶é—´æ ¼å¼ä¸æ­£ç¡®ã€‚è¯·ä½¿ç”¨ 'YYYY-MM-DDTHH:MM:SS' æ ¼å¼ã€‚"
            )
            exit(1)
        generator.run(
            start_time=start_dt_utc,
            end_time=end_dt_utc,
            attachment_data=attachment_data,
            attachment_type=attachment_type,
        )
