# main.py
import os
import json
import argparse
import difflib
import sqlite3
import platform
from pathlib import Path
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional

# --- åŠ¨æ€å¯¼å…¥æ£€æŸ¥ ---
try:
    import tiktoken
    from sentence_transformers import SentenceTransformer, util
    import google.generativeai as genai
    from openai import OpenAI
    import transformers
except ImportError as e:
    print(f"ä¾èµ–åº“å¯¼å…¥å¤±è´¥: {e}")
    print("è¯·ç¡®ä¿å·²é€šè¿‡ 'pip install -r requirements.txt' å®‰è£…æ‰€æœ‰ä¾èµ–ã€‚")
    exit(1)


# --- æ¨¡å— 1.6: LLM æ¥å£æ¨¡å— (å·²å‡çº§) ---
class LLMConnector:
    def __init__(self, provider_name: str, provider_config: Dict[str, Any]):
        self.provider_name = provider_name
        self.provider_config = provider_config
        self.client = self._initialize_client()
        print(
            f"ğŸ¤– LLM è¿æ¥å™¨å·²ä¸º provider åˆå§‹åŒ–: {self.provider_name} (æ¨¡å‹: {self.provider_config.get('model_name')})"
        )

    def _initialize_client(self):
        if self.provider_name == "gemini":
            proxy = self.provider_config.get("proxy")
            if proxy:
                os.environ["HTTP_PROXY"] = proxy
                os.environ["HTTPS_PROXY"] = proxy
                print(f"ğŸ”§ å·²ä¸º Gemini è®¾ç½®ç½‘ç»œä»£ç†: {proxy}")

            genai.configure(api_key=self.provider_config["api_key"])
            return genai.GenerativeModel(self.provider_config["model_name"])

        elif self.provider_name == "deepseek":
            return OpenAI(
                api_key=self.provider_config["api_key"],
                base_url=self.provider_config["base_url"],
            )
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„ LLM provider: {self.provider_name}")

    def generate(
        self,
        user_prompt: str,
        system_prompt: Optional[str] = None,
        temperature: float = 0.7,
    ) -> str:
        print(f"\nğŸš€ å‘èµ·çœŸå® LLM API è°ƒç”¨ (Temperature: {temperature})...")
        try:
            if self.provider_name == "gemini":
                # Gemini é€šè¿‡ GenerationConfig ä¼ é€’ temperature
                generation_config = genai.types.GenerationConfig(
                    temperature=temperature
                )
                # å°† system_prompt (å¦‚æœå­˜åœ¨) é™„åŠ åˆ° user_prompt å‰é¢
                full_prompt = (
                    f"{system_prompt}\n\n---\n\n{user_prompt}"
                    if system_prompt
                    else user_prompt
                )
                response = self.client.generate_content(
                    full_prompt, generation_config=generation_config
                )
                return response.text

            elif self.provider_name == "deepseek":
                messages = []
                if system_prompt:
                    messages.append({"role": "system", "content": system_prompt})
                messages.append({"role": "user", "content": user_prompt})

                chat_completion = self.client.chat.completions.create(
                    messages=messages,
                    model=self.provider_config["model_name"],
                    temperature=temperature,
                )
                return chat_completion.choices[0].message.content

        except Exception as e:
            print(f"âŒ LLM API è°ƒç”¨å¤±è´¥: {e}")
            return f"[LLM è°ƒç”¨é”™è¯¯: {e}]"
        return "[LLM è°ƒç”¨è¿”å›ç©º]"


# --- æ¨¡å— 1.1: æ•°æ®è·å–æ¨¡å— ---
class DataFetcher:
    # ... æ­¤éƒ¨åˆ†ä»£ç æ— å˜åŒ– ...
    def __init__(self):
        self.db_path = self._get_db_path()
        if not self.db_path or not self.db_path.exists():
            print(f"âŒ é”™è¯¯: æœªèƒ½æ‰¾åˆ° Screenpipe æ•°æ®åº“ã€‚é¢„æœŸè·¯å¾„: {self.db_path}")
            exit(1)
        print(f"ğŸ” æˆåŠŸå®šä½ Screenpipe æ•°æ®åº“: {self.db_path}")

    def _get_db_path(self) -> Path | None:
        system = platform.system()
        home = Path.home()
        if system == "Windows":
            path1 = home / ".screenpipe/db.sqlite"
            path2 = home / "AppData/Roaming/Screenpipe/db.sqlite"
            if path1.exists():
                return path1
            return path2
        if system == "Darwin":
            return home / "Library/Application Support/Screenpipe/db.sqlite"
        if system == "Linux":
            return home / ".config/Screenpipe/db.sqlite"
        return None

    def fetch_data(
        self, start_time: datetime, end_time: datetime
    ) -> List[Dict[str, Any]]:
        print(
            f"ğŸ’¾ æ­£åœ¨ä»æ•°æ®åº“è·å– {start_time.isoformat()} åˆ° {end_time.isoformat()} çš„ OCR æ•°æ®..."
        )
        query = "SELECT ocr.text, frm.timestamp AS captured_at FROM frames AS frm JOIN ocr_text AS ocr ON frm.id = ocr.frame_id WHERE frm.timestamp >= ? AND frm.timestamp <= ? ORDER BY frm.timestamp ASC;"
        records = []
        try:
            with sqlite3.connect(f"file:{self.db_path}?mode=ro", uri=True) as conn:
                conn.row_factory = sqlite3.Row
                cursor = conn.cursor()
                cursor.execute(query, (start_time.isoformat(), end_time.isoformat()))
                for row in cursor.fetchall():
                    records.append(dict(row))
            print(f"âœ… æˆåŠŸè·å– {len(records)} æ¡ OCR è®°å½•ã€‚")
            return records
        except sqlite3.Error as e:
            print(f"âŒ æ•°æ®åº“æŸ¥è¯¢å¤±è´¥: {e}")
            return []


# --- æ ¸å¿ƒåº”ç”¨ç±» (å·²å‡çº§) ---
class DailyReportGenerator:
    def __init__(self, config_path: str, cli_args: argparse.Namespace):
        self.config = self._load_config(config_path)
        self.data_fetcher = DataFetcher()
        self.cli_args = cli_args

        # å†³å®š LLM Provider
        if cli_args.llm and cli_args.llm in self.config["llm_config"]:
            self.llm_provider_name = cli_args.llm
            print(f"ğŸ”§ å·²é€šè¿‡å‘½ä»¤è¡Œå‚æ•°é€‰æ‹© LLM: {self.llm_provider_name}")
        else:
            self.llm_provider_name = self.config["llm_provider"]
            if cli_args.llm:
                print(
                    f"âš ï¸ è­¦å‘Š: å‘½ä»¤è¡ŒæŒ‡å®šçš„ LLM '{cli_args.llm}' åœ¨ config.json ä¸­æœªé…ç½®ï¼Œå°†ä½¿ç”¨é»˜è®¤å€¼: '{self.llm_provider_name}'"
                )
        self.provider_config = self.config["llm_config"][self.llm_provider_name]

        # åˆå§‹åŒ– LLM è¿æ¥å™¨
        self.llm_connector = LLMConnector(
            provider_name=self.llm_provider_name, provider_config=self.provider_config
        )

        # *** å…³é”®å˜æ›´: åŠ è½½æŒ‡å®šçš„ä»»åŠ¡æ¨¡æ¿ ***
        self.task_template = self._load_task_template(cli_args.task)

        # *** å…³é”®å˜æ›´: å†³å®šæœ€ç»ˆçš„ Temperature ***
        # ä¼˜å…ˆçº§: å‘½ä»¤è¡Œ > é…ç½®æ–‡ä»¶
        if cli_args.temperature is not None:
            self.temperature = cli_args.temperature
            print(f"ğŸ”§ å·²é€šè¿‡å‘½ä»¤è¡Œå‚æ•°è®¾ç½® Temperature: {self.temperature}")
        else:
            self.temperature = self.provider_config.get("temperature", 0.7)  # é»˜è®¤ 0.7

        # åŠ è½½ NLP æ¨¡å‹å’Œ Tokenizer
        print("æ­£åœ¨åŠ è½½ NLP æ¨¡å‹...")
        self.similarity_model = SentenceTransformer("all-MiniLM-L6-v2")
        self.tokenizer = self._load_tokenizer()
        print("æ¨¡å‹åŠ è½½å®Œæˆã€‚")

    def _load_config(self, path: str) -> Dict[str, Any]:
        try:
            with open(path, "r", encoding="utf-8") as f:
                return json.load(f)
        except FileNotFoundError:
            print(f"é”™è¯¯: é…ç½®æ–‡ä»¶ '{path}' æœªæ‰¾åˆ°ã€‚")
            exit(1)
        except json.JSONDecodeError:
            print(f"é”™è¯¯: é…ç½®æ–‡ä»¶ '{path}' æ ¼å¼ä¸æ­£ç¡®ã€‚")
            exit(1)

    def _load_task_template(self, task_name: str) -> Dict[str, str]:
        templates = self.config.get("prompt_templates", {})
        if task_name not in templates:
            print(f"âŒ é”™è¯¯: ä»»åŠ¡ '{task_name}' åœ¨é…ç½®æ–‡ä»¶ä¸­æœªå®šä¹‰ã€‚")
            print("å¯ç”¨çš„ä»»åŠ¡æœ‰:", list(templates.keys()))
            exit(1)
        print(
            f"ğŸš€ å·²é€‰æ‹©ä»»åŠ¡: '{task_name}' - {templates[task_name].get('description')}"
        )
        return templates[task_name]

    def _load_tokenizer(self):
        if self.llm_provider_name == "deepseek":
            tokenizer_path = self.provider_config.get("tokenizer_path")
            if tokenizer_path and os.path.isdir(tokenizer_path):
                try:
                    print(
                        f"  - æ­£åœ¨ä»æœ¬åœ°è·¯å¾„åŠ è½½ DeepSeek Tokenizer: {tokenizer_path}"
                    )
                    return transformers.AutoTokenizer.from_pretrained(
                        tokenizer_path, trust_remote_code=True
                    )
                except Exception as e:
                    print(
                        f"  - âŒ é”™è¯¯: åŠ è½½ DeepSeek Tokenizer å¤±è´¥: {e}ã€‚å°†å›é€€åˆ° tiktokenã€‚"
                    )
        print("  - æ­£åœ¨åŠ è½½ tiktoken ä½œä¸ºé€šç”¨ Tokenizerã€‚")
        return tiktoken.get_encoding("cl100k_base")

    def _clean_data(self, records: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        # ... æ­¤éƒ¨åˆ†ä»£ç æ— å˜åŒ– ...
        if not records:
            return []
        print("ğŸ§¹ å¼€å§‹æ•°æ®æ¸…æ´—...")
        high_similarity_threshold = self.config["high_similarity_threshold"]
        min_diff_chars = self.config["min_diff_chars"]
        first_valid_index = next(
            (i for i, r in enumerate(records) if r.get("text")), -1
        )
        if first_valid_index == -1:
            return []
        cleaned_records = [records[first_valid_index]]
        for i in range(first_valid_index + 1, len(records)):
            current_record, last_kept_record = records[i], cleaned_records[-1]
            text_current, text_last_kept = current_record.get(
                "text", ""
            ), last_kept_record.get("text", "")
            if not text_current:
                continue
            embeddings = self.similarity_model.encode([text_last_kept, text_current])
            if (
                util.cos_sim(embeddings[0], embeddings[1]).item()
                > high_similarity_threshold
            ):
                continue
            diff = difflib.ndiff(
                text_last_kept.splitlines(keepends=True),
                text_current.splitlines(keepends=True),
            )
            if (
                sum(len(line[2:]) for line in diff if line.startswith("+ "))
                < min_diff_chars
            ):
                continue
            cleaned_records.append(current_record)
        print(
            f"âœ… æ¸…æ´—å®Œæˆã€‚è®°å½•ä» {len(records)} æ¡å‡å°‘åˆ° {len(cleaned_records)} æ¡ã€‚"
        )
        return cleaned_records

    def _estimate_tokens(self, text: str) -> int:
        return len(self.tokenizer.encode(text))

    def run(self, start_time: datetime, end_time: datetime):
        raw_records = self.data_fetcher.fetch_data(start_time, end_time)
        cleaned_records = self._clean_data(raw_records)
        if not cleaned_records:
            print("â„¹ï¸ æ¸…æ´—åæ— æœ‰æ•ˆæ•°æ®ï¼Œæ— æ³•ç”ŸæˆæŠ¥å‘Šã€‚")
            return

        full_text = "\n\n---\n\n".join(
            [f"Timestamp: {r['captured_at']}\n{r['text']}" for r in cleaned_records]
        )
        total_tokens = self._estimate_tokens(full_text)
        print(f"ğŸ“Š æ¸…æ´—åæ€» Token æ•°ä¼°ç®—: {total_tokens}")

        llm_context = ""
        llm_context_window = self.provider_config["context_window"]
        effective_direct_summary_threshold = min(
            self.config["direct_summary_threshold"], llm_context_window
        )

        if total_tokens < effective_direct_summary_threshold:
            print("ğŸ“ˆ Token æ€»æ•°å°äºé˜ˆå€¼ï¼Œç›´æ¥ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Šã€‚")
            llm_context = full_text
        else:
            print("ğŸ“‰ Token æ€»æ•°è¶…è¿‡é˜ˆå€¼ï¼Œæ‰§è¡Œåˆ†æ®µæ‘˜è¦æµç¨‹ã€‚")
            available_space = llm_context_window - self.config["token_headroom"]
            if self.llm_provider_name == "deepseek" and isinstance(
                self.tokenizer, transformers.PreTrainedTokenizerFast
            ):
                max_chunk_tokens = available_space
                print(f"  - ä½¿ç”¨ç²¾ç¡®çš„ DeepSeek Tokenizerï¼Œæ— éœ€å®‰å…¨ç³»æ•°ã€‚")
            else:
                safety_margin = self.config.get("tokenizer_safety_margin", 1.0)
                max_chunk_tokens = int(available_space * safety_margin)
                print(f"  - ä½¿ç”¨ tiktoken ä¼°ç®—ï¼Œåº”ç”¨å®‰å…¨ç³»æ•° ({safety_margin})...")
            print(f"  - æ¯ä¸ªåˆ†æ®µæœ€å¤§ Token ä¸Šé™: {max_chunk_tokens}")

            chunks, current_chunk_text, current_chunk_tokens = [], "", 0
            for record in cleaned_records:
                record_text = f"Timestamp: {record['captured_at']}\n{record['text']}"
                record_tokens = self._estimate_tokens(record_text)
                if (
                    current_chunk_tokens + record_tokens > max_chunk_tokens
                    and current_chunk_text
                ):
                    chunks.append(current_chunk_text)
                    current_chunk_text, current_chunk_tokens = "", 0
                current_chunk_text += record_text + "\n\n---\n\n"
                current_chunk_tokens += record_tokens
            if current_chunk_text:
                chunks.append(current_chunk_text)

            print(f"  - æ•°æ®è¢«åˆ†ä¸º {len(chunks)} ä¸ªæ®µè½ã€‚")

            # *** å…³é”®å˜æ›´: ä½¿ç”¨æ¨¡æ¿ä¸­çš„æç¤ºè¯ ***
            chunk_prompt_template = self.task_template["chunk_summary_prompt"]
            system_prompt = self.task_template.get("system_prompt")

            summaries = [
                self.llm_connector.generate(
                    user_prompt=chunk_prompt_template.format(chunk_text=chunk),
                    system_prompt=system_prompt,
                    temperature=self.temperature,
                )
                for chunk in chunks
            ]
            llm_context = "\n\n".join(summaries)

        print("ğŸ–‹ï¸ å¼€å§‹ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š...")
        final_prompt_template = self.task_template["final_report_prompt"]
        final_report = self.llm_connector.generate(
            user_prompt=final_prompt_template.format(all_summaries=llm_context),
            system_prompt=system_prompt,
            temperature=self.temperature,
        )

        # *** å…³é”®å˜æ›´: ç”ŸæˆåŠ¨æ€æ–‡ä»¶å ***
        output_dir = self.config["output_path"]
        os.makedirs(output_dir, exist_ok=True)
        timestamp_str = datetime.now().strftime("%Y-%m-%d_%H%M%S")
        report_filename = (
            f"{timestamp_str}_{self.cli_args.task}_{self.llm_provider_name}.md"
        )
        report_path = os.path.join(output_dir, report_filename)

        with open(report_path, "w", encoding="utf-8") as f:
            f.write(final_report)
        print(f"\nğŸ‰ æˆåŠŸï¼æŠ¥å‘Šå·²ä¿å­˜è‡³: {report_path}")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Screenpipe OCR æ™ºèƒ½æ—¥æŠ¥ç”ŸæˆåŠ©æ‰‹",
        formatter_class=argparse.RawTextHelpFormatter,
    )
    parser.add_argument(
        "--start_time",
        type=str,
        help="å¼€å§‹æ—¶é—´ (æ ¼å¼: YYYY-MM-DDTHH:MM:SS)ï¼Œé»˜è®¤ä¸º24å°æ—¶å‰",
        default=(datetime.now() - timedelta(days=1)).isoformat(timespec="seconds"),
    )
    parser.add_argument(
        "--end_time",
        type=str,
        help="ç»“æŸæ—¶é—´ (æ ¼å¼: YYYY-MM-DDTHH:MM:SS)ï¼Œé»˜è®¤ä¸ºå½“å‰æ—¶é—´",
        default=datetime.now().isoformat(timespec="seconds"),
    )
    parser.add_argument(
        "--config", type=str, default="config.json", help="é…ç½®æ–‡ä»¶çš„è·¯å¾„"
    )
    parser.add_argument(
        "--llm",
        type=str,
        help="é€‰æ‹©ä½¿ç”¨çš„å¤§è¯­è¨€æ¨¡å‹ (ä¾‹å¦‚: gemini, deepseek)ã€‚ä¼šè¦†ç›–é…ç½®æ–‡ä»¶ä¸­çš„è®¾ç½®ã€‚",
    )

    # *** æ–°å¢å‘½ä»¤è¡Œå‚æ•° ***
    parser.add_argument(
        "--task",
        type=str,
        default="daily_report",
        help="é€‰æ‹©è¦æ‰§è¡Œçš„ä»»åŠ¡ (å¯¹åº” config.json ä¸­çš„ prompt_templates key)ã€‚\nä¾‹å¦‚: daily_report, tutorial_generator",
    )
    parser.add_argument(
        "--temperature",
        type=float,
        help="è®¾ç½® LLM çš„ Temperatureã€‚ä¼šè¦†ç›–é…ç½®æ–‡ä»¶ä¸­çš„è®¾ç½®ã€‚",
    )

    args = parser.parse_args()

    try:
        start_dt, end_dt = datetime.fromisoformat(
            args.start_time
        ), datetime.fromisoformat(args.end_time)
    except ValueError:
        print("é”™è¯¯: æ—¶é—´æ ¼å¼ä¸æ­£ç¡®ã€‚è¯·ä½¿ç”¨ 'YYYY-MM-DDTHH:MM:SS' æ ¼å¼ã€‚")
        exit(1)

    generator = DailyReportGenerator(config_path=args.config, cli_args=args)
    generator.run(start_time=start_dt, end_time=end_dt)
